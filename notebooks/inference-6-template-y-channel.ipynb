{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "2b62145f",
      "metadata": {
        "id": "2b62145f"
      },
      "source": [
        "# Testing The Model\n",
        "\n",
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "U4YTHB921okR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U4YTHB921okR",
        "outputId": "5f10297d-ffdb-4c8e-e328-0440c42325e2"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"takihasan/div2k-dataset-for-super-resolution\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "767891b4",
      "metadata": {
        "id": "767891b4"
      },
      "outputs": [],
      "source": [
        "TEST_DIR = \"/kaggle/input/div2k-dataset-for-super-resolution/Dataset/DIV2K_valid_HR\"\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d66c8a3d",
      "metadata": {
        "id": "d66c8a3d"
      },
      "outputs": [],
      "source": [
        "# models.py\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    A single residual block as defined in the SRGAN paper.\n",
        "    It contains two convolutional layers with batch normalization and PReLU activation.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.conv_block1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "            nn.PReLU(),\n",
        "        )\n",
        "        self.conv_block2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(in_channels),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.conv_block1(x)\n",
        "        out = self.conv_block2(out)\n",
        "        return identity + out\n",
        "\n",
        "class UpsampleBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Upsampling block using a convolutional layer and PixelShuffle.\n",
        "    This increases the resolution by a factor of 2.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, scale_factor=2):\n",
        "        super(UpsampleBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, in_channels * (scale_factor ** 2), kernel_size=3, stride=1, padding=1)\n",
        "        self.pixel_shuffle = nn.PixelShuffle(scale_factor)\n",
        "        self.prelu = nn.PReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.prelu(self.pixel_shuffle(self.conv(x)))\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    \"\"\"\n",
        "    The Generator Network (SRResNet).\n",
        "    It takes a low-resolution image and outputs a super-resolved version.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=3, num_res_blocks=16):\n",
        "        super(Generator, self).__init__()\n",
        "        self.initial_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=9, stride=1, padding=4),\n",
        "            nn.PReLU()\n",
        "        )\n",
        "\n",
        "        self.residuals = nn.Sequential(*[ResidualBlock(64) for _ in range(num_res_blocks)])\n",
        "\n",
        "        self.mid_conv = nn.Sequential(\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64)\n",
        "        )\n",
        "\n",
        "        # Upsampling by 4x (two 2x upsample blocks)\n",
        "        self.upsample_blocks = nn.Sequential(\n",
        "            UpsampleBlock(64),\n",
        "            UpsampleBlock(64),\n",
        "        )\n",
        "\n",
        "        self.final_conv = nn.Conv2d(64, in_channels, kernel_size=9, stride=1, padding=4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        initial_out = self.initial_conv(x)\n",
        "        residual_out = self.residuals(initial_out)\n",
        "        mid_out = self.mid_conv(residual_out)\n",
        "        mid_out = mid_out + initial_out # Skip connection\n",
        "        upsampled_out = self.upsample_blocks(mid_out)\n",
        "        final_out = self.final_conv(upsampled_out)\n",
        "        return torch.tanh(final_out) # Tanh activation to scale output to [-1, 1]\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    The Discriminator Network.\n",
        "    It takes an image and outputs a probability of it being a real high-resolution image.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels=3):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        def conv_block(in_feat, out_feat, stride=1):\n",
        "            return nn.Sequential(\n",
        "                nn.Conv2d(in_feat, out_feat, kernel_size=3, stride=stride, padding=1),\n",
        "                nn.BatchNorm2d(out_feat),\n",
        "                nn.LeakyReLU(0.2, inplace=True)\n",
        "            )\n",
        "\n",
        "        self.blocks = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "\n",
        "            conv_block(64, 64, stride=2),\n",
        "            conv_block(64, 128, stride=1),\n",
        "            conv_block(128, 128, stride=2),\n",
        "            conv_block(128, 256, stride=1),\n",
        "            conv_block(256, 256, stride=2),\n",
        "            conv_block(256, 512, stride=1),\n",
        "            conv_block(512, 512, stride=2),\n",
        "        )\n",
        "\n",
        "        # The paper mentions flattening and then two dense layers\n",
        "        # The output size after convolutions on a 96x96 image is 512x6x6\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.AdaptiveAvgPool2d(1), # Flattens the output\n",
        "            nn.Conv2d(512, 1024, kernel_size=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(1024, 1, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        out = self.blocks(x)\n",
        "        out = self.classifier(out)\n",
        "        return out.view(batch_size, -1) # No sigmoid here, handled by BCEWithLogitsLoss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af64d228",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "af64d228",
        "outputId": "96f26c3a-e788-4f37-cbc0-278b55ae8755"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "# Hugging Face model configuration\n",
        "HF_MODEL_NAME = \"keanteng/srgan-div2k-0723\"  # Your model name from srgan.ipynb\n",
        "\n",
        "# Download the generator model\n",
        "GEN_PATH = hf_hub_download(\n",
        "    repo_id=HF_MODEL_NAME,\n",
        "    filename=\"generator.pth\"\n",
        ")\n",
        "\n",
        "print(f\"Downloaded model to: {GEN_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c83df25",
      "metadata": {
        "id": "4c83df25"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c86f64eb",
      "metadata": {
        "id": "c86f64eb"
      },
      "outputs": [],
      "source": [
        "# evaluate.py\n",
        "from torchvision.utils import save_image\n",
        "from torchvision import transforms\n",
        "import cv2\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "import numpy as np\n",
        "#import config\n",
        "#from models import Generator\n",
        "\n",
        "def calculate_psnr(img1, img2):\n",
        "    \"\"\"\n",
        "    Calculate PSNR between two images.\n",
        "    Images should be in range [0, 255] and of type uint8.\n",
        "    \"\"\"\n",
        "    if img1.shape != img2.shape:\n",
        "        raise ValueError(\"Input images must have the same dimensions\")\n",
        "\n",
        "    mse = np.mean((img1.astype(np.float64) - img2.astype(np.float64)) ** 2)\n",
        "    if mse == 0:\n",
        "        return float('inf')\n",
        "\n",
        "    max_pixel = 255.0\n",
        "    psnr_value = 20 * np.log10(max_pixel / np.sqrt(mse))\n",
        "    return psnr_value\n",
        "\n",
        "def calculate_ssim(img1, img2):\n",
        "    \"\"\"\n",
        "    Calculate SSIM between two images.\n",
        "    Images should be in range [0, 255] and of type uint8.\n",
        "    \"\"\"\n",
        "    if img1.shape != img2.shape:\n",
        "        raise ValueError(\"Input images must have the same dimensions\")\n",
        "\n",
        "    # Convert to grayscale if images are color\n",
        "    if len(img1.shape) == 3:\n",
        "        img1_gray = cv2.cvtColor(img1, cv2.COLOR_RGB2GRAY)\n",
        "        img2_gray = cv2.cvtColor(img2, cv2.COLOR_RGB2GRAY)\n",
        "    else:\n",
        "        img1_gray = img1\n",
        "        img2_gray = img2\n",
        "\n",
        "    ssim_value = ssim(img1_gray, img2_gray, data_range=255)\n",
        "    return ssim_value\n",
        "\n",
        "def tensor_to_numpy(tensor):\n",
        "    \"\"\"\n",
        "    Convert tensor to numpy array in range [0, 255].\n",
        "    \"\"\"\n",
        "    # Denormalize from [-1, 1] to [0, 1]\n",
        "    tensor = tensor * 0.5 + 0.5\n",
        "    # Clamp to [0, 1]\n",
        "    tensor = torch.clamp(tensor, 0, 1)\n",
        "    # Convert to numpy and scale to [0, 255]\n",
        "    numpy_img = tensor.squeeze(0).cpu().detach().numpy()\n",
        "    numpy_img = np.transpose(numpy_img, (1, 2, 0))  # CHW to HWC\n",
        "    numpy_img = (numpy_img * 255).astype(np.uint8)\n",
        "    return numpy_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa34b01e",
      "metadata": {
        "id": "fa34b01e"
      },
      "outputs": [],
      "source": [
        "def rgb_to_y_channel(img):\n",
        "    \"\"\"Convert RGB image to Y channel (luminance)\"\"\"\n",
        "    if len(img.shape) == 3:\n",
        "        # ITU-R BT.601 conversion\n",
        "        y = 0.299 * img[:, :, 0] + 0.587 * img[:, :, 1] + 0.114 * img[:, :, 2]\n",
        "        return y.astype(np.uint8)\n",
        "    return img\n",
        "\n",
        "def calculate_metrics_paper_protocol(img1, img2):\n",
        "    \"\"\"\n",
        "    Calculate PSNR and SSIM following the original SRGAN paper protocol\n",
        "    \"\"\"\n",
        "    # Convert to Y channel\n",
        "    y1 = rgb_to_y_channel(img1)\n",
        "    y2 = rgb_to_y_channel(img2)\n",
        "\n",
        "    # Remove 4-pixel border (center crop)\n",
        "    h, w = y1.shape\n",
        "    y1_cropped = y1[4:h-4, 4:w-4]\n",
        "    y2_cropped = y2[4:h-4, 4:w-4]\n",
        "\n",
        "    # Calculate metrics on Y channel only\n",
        "    psnr_val = calculate_psnr(y1_cropped, y2_cropped)\n",
        "    ssim_val = ssim(y1_cropped, y2_cropped, data_range=255)\n",
        "\n",
        "    return psnr_val, ssim_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ee9671b",
      "metadata": {
        "id": "3ee9671b"
      },
      "outputs": [],
      "source": [
        "def test_original_image(image_path, save_prefix=\"original\"):\n",
        "    \"\"\"\n",
        "    Test the model on an original image without any preprocessing.\n",
        "    The image will be processed in patches if it's too large.\n",
        "    \"\"\"\n",
        "    # Load the original image\n",
        "    original_image = Image.open(image_path).convert(\"RGB\")\n",
        "    original_width, original_height = original_image.size\n",
        "\n",
        "    print(f\"Original image size: {original_width} x {original_height}\")\n",
        "\n",
        "    # For very large images, we might need to process in patches\n",
        "    # For demonstration, let's resize to a manageable size first\n",
        "    max_size = 512  # Adjust based on your GPU memory\n",
        "\n",
        "    if max(original_width, original_height) > max_size:\n",
        "        # Resize while maintaining aspect ratio\n",
        "        ratio = min(max_size / original_width, max_size / original_height)\n",
        "        new_width = int(original_width * ratio)\n",
        "        new_height = int(original_height * ratio)\n",
        "        resized_image = original_image.resize((new_width, new_height), Image.BICUBIC)\n",
        "        print(f\"Resized to: {new_width} x {new_height}\")\n",
        "    else:\n",
        "        resized_image = original_image\n",
        "        new_width, new_height = original_width, original_height\n",
        "\n",
        "    # Ensure dimensions are divisible by 4 (due to 4x upscaling)\n",
        "    adjusted_width = (new_width // 4) * 4\n",
        "    adjusted_height = (new_height // 4) * 4\n",
        "\n",
        "    if adjusted_width != new_width or adjusted_height != new_height:\n",
        "        resized_image = resized_image.resize((adjusted_width, adjusted_height), Image.BICUBIC)\n",
        "        print(f\"Adjusted to: {adjusted_width} x {adjusted_height}\")\n",
        "\n",
        "    # Create LR version by downsampling by 4x\n",
        "    lr_width = adjusted_width // 4\n",
        "    lr_height = adjusted_height // 4\n",
        "    lr_image = resized_image.resize((lr_width, lr_height), Image.BICUBIC)\n",
        "\n",
        "    # Convert to tensor and normalize\n",
        "    lr_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0]),\n",
        "    ])\n",
        "    lr_tensor = lr_transform(lr_image).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "    # Generate SR image\n",
        "    gen = Generator().to(DEVICE)\n",
        "    gen.load_state_dict(torch.load(GEN_PATH, map_location=DEVICE))\n",
        "    gen.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        sr_tensor = gen(lr_tensor)\n",
        "\n",
        "    # Convert back to PIL images\n",
        "    sr_image = transforms.ToPILImage()((sr_tensor.cpu().squeeze(0) * 0.5 + 0.5).clamp(0, 1))\n",
        "\n",
        "    # Create bicubic upscaled version for comparison\n",
        "    bicubic_image = lr_image.resize((adjusted_width, adjusted_height), Image.BICUBIC)\n",
        "\n",
        "    # Save results\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "    sr_image.save(f\"results/{save_prefix}_sr_result.png\")\n",
        "    bicubic_image.save(f\"results/{save_prefix}_bicubic_result.png\")\n",
        "    lr_image.save(f\"results/{save_prefix}_lr_input.png\")\n",
        "    resized_image.save(f\"results/{save_prefix}_original.png\")\n",
        "\n",
        "    # Display comparison\n",
        "    plt.figure(figsize=(20, 5))\n",
        "\n",
        "    plt.subplot(1, 4, 1)\n",
        "    plt.title(\"Low Resolution Input\")\n",
        "    plt.imshow(lr_image)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 4, 2)\n",
        "    plt.title(\"Bicubic Upscaling\")\n",
        "    plt.imshow(bicubic_image)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 4, 3)\n",
        "    plt.title(\"SRGAN Output\")\n",
        "    plt.imshow(sr_image)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 4, 4)\n",
        "    plt.title(\"Original (Ground Truth)\")\n",
        "    plt.imshow(resized_image)\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return lr_image, bicubic_image, sr_image, resized_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63c85a0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "63c85a0d",
        "outputId": "8d312cc8-cd4c-4db0-acfb-8e32f23c9d34"
      },
      "outputs": [],
      "source": [
        "# Test on an original image\n",
        "test_image_path = f\"{TEST_DIR}/0801.png\"  # Use any image path\n",
        "lr_img, bicubic_img, sr_img, original_img = test_original_image(test_image_path, \"test1\")\n",
        "\n",
        "# Calculate metrics if needed\n",
        "sr_numpy = np.array(sr_img)\n",
        "original_numpy = np.array(original_img)\n",
        "bicubic_numpy = np.array(bicubic_img)\n",
        "\n",
        "sr_psnr, sr_ssim = calculate_metrics_paper_protocol(original_numpy, sr_numpy)\n",
        "bicubic_psnr, bicubic_ssim = calculate_metrics_paper_protocol(original_numpy, bicubic_numpy)\n",
        "\n",
        "print(\"=== Results on Original Image ===\")\n",
        "print(f\"SRGAN vs Original:\")\n",
        "print(f\"  PSNR: {sr_psnr:.2f} dB\")\n",
        "print(f\"  SSIM: {sr_ssim:.4f}\")\n",
        "print(f\"\\nBicubic vs Original:\")\n",
        "print(f\"  PSNR: {bicubic_psnr:.2f} dB\")\n",
        "print(f\"  SSIM: {bicubic_ssim:.4f}\")\n",
        "print(f\"\\nImprovement:\")\n",
        "print(f\"  PSNR: +{sr_psnr - bicubic_psnr:.2f} dB\")\n",
        "print(f\"  SSIM: +{sr_ssim - bicubic_ssim:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eHzK6pN-3Awt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 622
        },
        "id": "eHzK6pN-3Awt",
        "outputId": "f575f418-f593-4569-bb8c-4cbf01445ddc"
      },
      "outputs": [],
      "source": [
        "# Test on an original image\n",
        "test_image_path = f\"{TEST_DIR}/0813.png\"  # Use any image path\n",
        "lr_img, bicubic_img, sr_img, original_img = test_original_image(test_image_path, \"test1\")\n",
        "\n",
        "# Calculate metrics if needed\n",
        "sr_numpy = np.array(sr_img)\n",
        "original_numpy = np.array(original_img)\n",
        "bicubic_numpy = np.array(bicubic_img)\n",
        "\n",
        "sr_psnr, sr_ssim = calculate_metrics_paper_protocol(original_numpy, sr_numpy)\n",
        "bicubic_psnr, bicubic_ssim = calculate_metrics_paper_protocol(original_numpy, bicubic_numpy)\n",
        "\n",
        "print(\"=== Results on Original Image ===\")\n",
        "print(f\"SRGAN vs Original:\")\n",
        "print(f\"  PSNR: {sr_psnr:.2f} dB\")\n",
        "print(f\"  SSIM: {sr_ssim:.4f}\")\n",
        "print(f\"\\nBicubic vs Original:\")\n",
        "print(f\"  PSNR: {bicubic_psnr:.2f} dB\")\n",
        "print(f\"  SSIM: {bicubic_ssim:.4f}\")\n",
        "print(f\"\\nImprovement:\")\n",
        "print(f\"  PSNR: +{sr_psnr - bicubic_psnr:.2f} dB\")\n",
        "print(f\"  SSIM: +{sr_ssim - bicubic_ssim:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hA_QCNWE3Jl7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 639
        },
        "id": "hA_QCNWE3Jl7",
        "outputId": "fa340ea5-29a1-4bf0-ff44-42545fc0d5de"
      },
      "outputs": [],
      "source": [
        "# Test on an original image\n",
        "test_image_path = f\"{TEST_DIR}/0818.png\"  # Use any image path\n",
        "lr_img, bicubic_img, sr_img, original_img = test_original_image(test_image_path, \"test1\")\n",
        "\n",
        "# Calculate metrics if needed\n",
        "sr_numpy = np.array(sr_img)\n",
        "original_numpy = np.array(original_img)\n",
        "bicubic_numpy = np.array(bicubic_img)\n",
        "\n",
        "sr_psnr, sr_ssim = calculate_metrics_paper_protocol(original_numpy, sr_numpy)\n",
        "bicubic_psnr, bicubic_ssim = calculate_metrics_paper_protocol(original_numpy, bicubic_numpy)\n",
        "\n",
        "print(\"=== Results on Original Image ===\")\n",
        "print(f\"SRGAN vs Original:\")\n",
        "print(f\"  PSNR: {sr_psnr:.2f} dB\")\n",
        "print(f\"  SSIM: {sr_ssim:.4f}\")\n",
        "print(f\"\\nBicubic vs Original:\")\n",
        "print(f\"  PSNR: {bicubic_psnr:.2f} dB\")\n",
        "print(f\"  SSIM: {bicubic_ssim:.4f}\")\n",
        "print(f\"\\nImprovement:\")\n",
        "print(f\"  PSNR: +{sr_psnr - bicubic_psnr:.2f} dB\")\n",
        "print(f\"  SSIM: +{sr_ssim - bicubic_ssim:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b70c97c5",
      "metadata": {
        "id": "b70c97c5"
      },
      "source": [
        "# Batch Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f6868ce",
      "metadata": {
        "id": "2f6868ce"
      },
      "outputs": [],
      "source": [
        "# dataset.py\n",
        "import os\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Custom dataset to load high-resolution images and create low-resolution counterparts.\n",
        "    \"\"\"\n",
        "    def __init__(self, hr_dir, hr_size):\n",
        "        super(ImageDataset, self).__init__()\n",
        "        self.hr_image_files = [os.path.join(hr_dir, f) for f in os.listdir(hr_dir)]\n",
        "        self.hr_size = hr_size\n",
        "\n",
        "        # Transform for the original image before cropping\n",
        "        self.initial_transform = transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "        # Normalization transforms\n",
        "        self.hr_normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "        self.lr_normalize = transforms.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # Load image\n",
        "        hr_image = Image.open(self.hr_image_files[index]).convert(\"RGB\")\n",
        "\n",
        "        # Convert to tensor first\n",
        "        hr_tensor = self.initial_transform(hr_image)\n",
        "\n",
        "        # Apply random crop to get consistent size\n",
        "        crop_transform = transforms.RandomCrop(self.hr_size)\n",
        "        hr_cropped = crop_transform(hr_tensor)\n",
        "\n",
        "        # Create LR version by downsampling the cropped HR image\n",
        "        lr_tensor = transforms.functional.resize(\n",
        "            hr_cropped,\n",
        "            size=self.hr_size // 4,\n",
        "            interpolation=transforms.InterpolationMode.BICUBIC\n",
        "        )\n",
        "\n",
        "        # Apply normalization\n",
        "        hr_normalized = self.hr_normalize(hr_cropped)\n",
        "        lr_normalized = self.lr_normalize(lr_tensor)\n",
        "\n",
        "        return lr_normalized, hr_normalized\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.hr_image_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caf76d25",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caf76d25",
        "outputId": "bc5dbfde-63b2-4da7-ee40-e959bf6d25c1"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "HIGH_RES_SIZE = 96\n",
        "\n",
        "# Create test dataset\n",
        "test_dataset = ImageDataset(hr_dir=TEST_DIR, hr_size=HIGH_RES_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=1)\n",
        "\n",
        "# Load generator\n",
        "gen = Generator().to(DEVICE)\n",
        "gen.load_state_dict(torch.load(GEN_PATH, map_location=DEVICE))\n",
        "gen.eval()\n",
        "\n",
        "total_sr_psnr = 0\n",
        "total_sr_ssim = 0\n",
        "total_bicubic_psnr = 0\n",
        "total_bicubic_ssim = 0\n",
        "num_images = 0\n",
        "\n",
        "print(\"Evaluating on test dataset...\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (lr, hr) in enumerate(test_loader):\n",
        "        lr = lr.to(DEVICE)\n",
        "        hr = hr.to(DEVICE)\n",
        "\n",
        "        # Generate SR image\n",
        "        sr = gen(lr)\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        sr_numpy = tensor_to_numpy(sr)\n",
        "        hr_numpy = tensor_to_numpy(hr)\n",
        "\n",
        "        # Create bicubic baseline\n",
        "        lr_numpy = tensor_to_numpy(lr)\n",
        "        bicubic_pil = Image.fromarray(lr_numpy).resize((HIGH_RES_SIZE, HIGH_RES_SIZE), Image.BICUBIC)\n",
        "        bicubic_numpy = np.array(bicubic_pil)\n",
        "\n",
        "        # Calculate metrics - Fixed: use hr_numpy instead of original_numpy\n",
        "        sr_psnr, sr_ssim = calculate_metrics_paper_protocol(hr_numpy, sr_numpy)\n",
        "        bicubic_psnr, bicubic_ssim = calculate_metrics_paper_protocol(hr_numpy, bicubic_numpy)\n",
        "\n",
        "        total_sr_psnr += sr_psnr\n",
        "        total_sr_ssim += sr_ssim\n",
        "        total_bicubic_psnr += bicubic_psnr\n",
        "        total_bicubic_ssim += bicubic_ssim\n",
        "        num_images += 1\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            print(f\"Processed {i + 1} images...\")\n",
        "\n",
        "# Calculate averages\n",
        "avg_sr_psnr = total_sr_psnr / num_images\n",
        "avg_sr_ssim = total_sr_ssim / num_images\n",
        "avg_bicubic_psnr = total_bicubic_psnr / num_images\n",
        "avg_bicubic_ssim = total_bicubic_ssim / num_images\n",
        "\n",
        "print(f\"\\n=== Average Results on {num_images} Test Images ===\")\n",
        "print(f\"SRGAN:\")\n",
        "print(f\"  Average PSNR: {avg_sr_psnr:.2f} dB\")\n",
        "print(f\"  Average SSIM: {avg_sr_ssim:.4f}\")\n",
        "print(f\"\\nBicubic Baseline:\")\n",
        "print(f\"  Average PSNR: {avg_bicubic_psnr:.2f} dB\")\n",
        "print(f\"  Average SSIM: {avg_bicubic_ssim:.4f}\")\n",
        "print(f\"\\nAverage Improvement:\")\n",
        "print(f\"  PSNR: +{avg_sr_psnr - avg_bicubic_psnr:.2f} dB\")\n",
        "print(f\"  SSIM: +{avg_sr_ssim - avg_bicubic_ssim:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
